{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98201379 0.98201379 0.98201379]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "\n",
    "weights = np.array([0.5, 0.5, 0.5])\n",
    "biases = np.array([1, 1, 1])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def __sigmoid__derivitive__(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "out = sigmoid(x @ weights + biases)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18449656, 0.29004648]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleLayer():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weight = np.random.uniform(low=-np.sqrt(1/input_dim), high=np.sqrt(1/input_dim), size=(input_dim, output_dim))\n",
    "        self.biases = np.random.uniform(low=-np.sqrt(1/input_dim), high=np.sqrt(1/input_dim), size=(output_dim, 1))\n",
    "\n",
    "    def params_nbr(self):\n",
    "        return self.input_dim * self.out_dim + self.out_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sigmoid(x @ self.weight + self.biases.T)\n",
    "\n",
    "\n",
    "layer1 = SimpleLayer(3, 2)\n",
    "layer1.forward(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprog goes here\n",
    "\n",
    "def mean_square_error(output, expected_output):\n",
    "    loss = np.mean((output - expected_output) ** 2)\n",
    "    grad = 2 * (output - expected_output) / output.size\n",
    "    return loss, grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = output_dim\n",
    "        self.weight = np.random.uniform(low=-np.sqrt(1/input_dim), high=np.sqrt(1/input_dim), size=(input_dim, output_dim))\n",
    "        self.biases = np.random.uniform(low=-np.sqrt(1/input_dim), high=np.sqrt(1/input_dim), size=(output_dim, 1))\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        self.last_z = None\n",
    "        self.gradient_cache = None\n",
    "\n",
    "    def backward(self, grad_output, lr):\n",
    "        # Compute gradient of the loss with respect to the linear combination (z)\n",
    "        grad_z = grad_output * sigmoid(self.last_z)\n",
    "        \n",
    "        # Compute gradients with respect to weights and biases\n",
    "        grad_weight = self.last_input.T @ grad_z\n",
    "        grad_bias = np.sum(grad_z, axis=0)\n",
    "        \n",
    "        # Reshape grad_bias to match the shape of self.biases\n",
    "        grad_bias = grad_bias.reshape(self.biases.shape)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weight -= lr * grad_weight\n",
    "        self.biases -= lr * grad_bias\n",
    "        \n",
    "        # Compute gradient to propagate to the previous layer\n",
    "        self.gradient_cache = grad_z @ self.weight.T\n",
    "        return self.gradient_cache\n",
    "\n",
    "    def params_nbr(self):\n",
    "        return self.input_dim * self.out_dim + self.out_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input = x  # x: (batch_size, input_dim)\n",
    "        self.last_z = x @ self.weight + self.biases.T  # Align dimensions\n",
    "        self.last_output = __sigmoid__derivitive__(self.last_z)\n",
    "        return self.last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(size, numbers):\n",
    "    array = []\n",
    "    for num in range(numbers):\n",
    "        r = np.random.randint(0, size)\n",
    "        \n",
    "        array.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.7332\n",
      "Epoch 11, Loss: 6.7068\n",
      "Epoch 21, Loss: 6.6847\n",
      "Epoch 31, Loss: 6.6692\n",
      "Epoch 41, Loss: 6.6624\n",
      "Epoch 51, Loss: 6.6653\n",
      "Epoch 61, Loss: 6.6771\n",
      "Epoch 71, Loss: 6.6946\n",
      "Epoch 81, Loss: 6.7124\n",
      "Epoch 91, Loss: 6.7255\n",
      "Epoch 101, Loss: 6.7309\n",
      "Epoch 111, Loss: 6.7288\n",
      "Epoch 121, Loss: 6.7226\n",
      "Epoch 131, Loss: 6.7187\n",
      "Epoch 141, Loss: 6.7257\n",
      "Epoch 151, Loss: 6.7535\n",
      "Epoch 161, Loss: 6.8097\n",
      "Epoch 171, Loss: 6.8943\n",
      "Epoch 181, Loss: 6.9957\n",
      "Epoch 191, Loss: 7.0958\n",
      "Final output: [[1.38057658e-02 9.28802791e-02 8.01689790e-07 4.34943364e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Input and output data\n",
    "x = np.array([1, 2, 3, 4]).reshape(1, -1)  # Shape (1, 4)\n",
    "y = np.array([1, 2, 3, 4]).reshape(1, -1)  # Shape (1, 4)\n",
    "\n",
    "# Create a Linear Layer with no activation\n",
    "layer = Linear(4, 4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    out = layer.forward(x)\n",
    "    loss, grad = mean_square_error(out, y)\n",
    "    layer.backward(grad, 1e-3)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final output\n",
    "print(\"Final output:\", layer.forward(np.array([2, 4, 6, 8]).reshape(1, -1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
